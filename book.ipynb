{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network for policy\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99  # Discount factor\n",
    "episodes = 2_000\n",
    "\n",
    "# Snake game\n",
    "input_dim = 12\n",
    "output_dim = 4\n",
    "\n",
    "# initialize policy network\n",
    "policy_network = PolicyNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = [danger + dir_one_hot + food_dir]\n",
    "from game import SnakeGame\n",
    "\n",
    "game = SnakeGame(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -10.19\n",
      "Episode 10, Total Reward: -10.3\n",
      "Episode 20, Total Reward: -10.73\n",
      "Episode 30, Total Reward: -10.15\n",
      "Episode 40, Total Reward: -10.38\n",
      "Episode 50, Total Reward: -10.19\n",
      "Episode 60, Total Reward: -11.14\n",
      "Episode 70, Total Reward: -10.93\n",
      "Episode 80, Total Reward: -11.010000000000002\n",
      "Episode 90, Total Reward: -10.23\n",
      "Episode 100, Total Reward: -10.940000000000001\n",
      "Episode 110, Total Reward: -10.51\n",
      "Episode 120, Total Reward: -10.780000000000001\n",
      "Episode 130, Total Reward: -10.26\n",
      "Episode 140, Total Reward: -11.15\n",
      "Episode 150, Total Reward: -10.700000000000001\n",
      "Episode 160, Total Reward: -10.33\n",
      "Episode 170, Total Reward: -10.52\n",
      "Episode 180, Total Reward: -12.0\n",
      "Episode 190, Total Reward: -10.59\n",
      "Episode 200, Total Reward: 9.29\n",
      "Episode 210, Total Reward: -10.450000000000001\n",
      "Episode 220, Total Reward: -10.73\n",
      "Episode 230, Total Reward: -1.7999999999999992\n",
      "Episode 240, Total Reward: -0.6400000000000002\n",
      "Episode 250, Total Reward: -1.1499999999999997\n",
      "Episode 260, Total Reward: -12.23\n",
      "Episode 270, Total Reward: 19.24\n",
      "Episode 280, Total Reward: 28.819999999999997\n",
      "Episode 290, Total Reward: -10.450000000000001\n",
      "Episode 300, Total Reward: -10.540000000000001\n",
      "Episode 310, Total Reward: -0.6799999999999994\n",
      "Episode 320, Total Reward: 8.780000000000001\n",
      "Episode 330, Total Reward: -0.9900000000000007\n",
      "Episode 340, Total Reward: -0.6\n",
      "Episode 350, Total Reward: -10.22\n",
      "Episode 360, Total Reward: 9.260000000000002\n",
      "Episode 370, Total Reward: -0.5600000000000003\n",
      "Episode 380, Total Reward: -0.19999999999999948\n",
      "Episode 390, Total Reward: 19.02\n",
      "Episode 400, Total Reward: -0.73\n",
      "Episode 410, Total Reward: 18.98\n",
      "Episode 420, Total Reward: -0.33999999999999975\n",
      "Episode 430, Total Reward: 19.03\n",
      "Episode 440, Total Reward: -0.27999999999999975\n",
      "Episode 450, Total Reward: 29.09\n",
      "Episode 460, Total Reward: 57.60000000000001\n",
      "Episode 470, Total Reward: -0.95\n",
      "Episode 480, Total Reward: -0.5199999999999998\n",
      "Episode 490, Total Reward: -11.040000000000001\n",
      "Episode 500, Total Reward: -0.24999999999999994\n",
      "Episode 510, Total Reward: 9.049999999999999\n",
      "Episode 520, Total Reward: -0.03999999999999915\n",
      "Episode 530, Total Reward: -0.46000000000000063\n",
      "Episode 540, Total Reward: 8.53\n",
      "Episode 550, Total Reward: 48.910000000000004\n",
      "Episode 560, Total Reward: 38.1\n",
      "Episode 570, Total Reward: 18.49\n",
      "Episode 580, Total Reward: -0.029999999999999576\n",
      "Episode 590, Total Reward: 38.97\n",
      "Episode 600, Total Reward: 39.04\n",
      "Episode 610, Total Reward: 57.58999999999999\n",
      "Episode 620, Total Reward: -0.7199999999999998\n",
      "Episode 630, Total Reward: 29.449999999999996\n",
      "Episode 640, Total Reward: -0.32999999999999996\n",
      "Episode 650, Total Reward: -0.20999999999999966\n",
      "Episode 660, Total Reward: 9.36\n",
      "Episode 670, Total Reward: 28.710000000000004\n",
      "Episode 680, Total Reward: 77.87\n",
      "Episode 690, Total Reward: 19.32\n",
      "Episode 700, Total Reward: 39.239999999999995\n",
      "Episode 710, Total Reward: 29.409999999999993\n",
      "Episode 720, Total Reward: -0.36999999999999994\n",
      "Episode 730, Total Reward: -10.18\n",
      "Episode 740, Total Reward: -10.44\n",
      "Episode 750, Total Reward: 9.589999999999998\n",
      "Episode 760, Total Reward: 48.56\n",
      "Episode 770, Total Reward: 48.809999999999995\n",
      "Episode 780, Total Reward: 38.720000000000006\n",
      "Episode 790, Total Reward: 38.370000000000005\n",
      "Episode 800, Total Reward: 58.5\n",
      "Episode 810, Total Reward: -0.11999999999999925\n",
      "Episode 820, Total Reward: 19.33\n",
      "Episode 830, Total Reward: -10.21\n",
      "Episode 840, Total Reward: 29.619999999999997\n",
      "Episode 850, Total Reward: 87.89\n",
      "Episode 860, Total Reward: 9.770000000000001\n",
      "Episode 870, Total Reward: 48.57\n",
      "Episode 880, Total Reward: 58.010000000000005\n",
      "Episode 890, Total Reward: 18.93\n",
      "Episode 900, Total Reward: -0.1799999999999997\n",
      "Episode 910, Total Reward: -0.14999999999999966\n",
      "Episode 920, Total Reward: 97.41000000000001\n",
      "Episode 930, Total Reward: 156.83\n",
      "Episode 940, Total Reward: 68.52\n",
      "Episode 950, Total Reward: 137.24999999999997\n",
      "Episode 960, Total Reward: 215.41\n",
      "Episode 970, Total Reward: 87.94\n",
      "Episode 980, Total Reward: 127.22999999999999\n",
      "Episode 990, Total Reward: 115.9\n",
      "Episode 1000, Total Reward: 165.92\n",
      "Episode 1010, Total Reward: 48.79\n",
      "Episode 1020, Total Reward: 97.08\n",
      "Episode 1030, Total Reward: 47.52\n",
      "Episode 1040, Total Reward: 173.32\n",
      "Episode 1050, Total Reward: 97.36999999999999\n",
      "Episode 1060, Total Reward: 107.33000000000001\n",
      "Episode 1070, Total Reward: 78.15\n",
      "Episode 1080, Total Reward: 67.49000000000001\n",
      "Episode 1090, Total Reward: 48.19\n",
      "Episode 1100, Total Reward: 96.38000000000001\n",
      "Episode 1110, Total Reward: 224.20000000000002\n",
      "Episode 1120, Total Reward: 58.62\n",
      "Episode 1130, Total Reward: 28.319999999999993\n",
      "Episode 1140, Total Reward: 311.44\n",
      "Episode 1150, Total Reward: 322.97\n",
      "Episode 1160, Total Reward: 38.690000000000005\n",
      "Episode 1170, Total Reward: 204.86\n",
      "Episode 1180, Total Reward: 292.25\n",
      "Episode 1190, Total Reward: 77.22\n",
      "Episode 1200, Total Reward: 96.82000000000001\n",
      "Episode 1210, Total Reward: 68.07\n",
      "Episode 1220, Total Reward: 304.82\n",
      "Episode 1230, Total Reward: 342.9\n",
      "Episode 1240, Total Reward: 313.33000000000004\n",
      "Episode 1250, Total Reward: 273.97\n",
      "Episode 1260, Total Reward: 283.74\n",
      "Episode 1270, Total Reward: 147.54999999999998\n",
      "Episode 1280, Total Reward: 196.10999999999999\n",
      "Episode 1290, Total Reward: 186.20000000000002\n",
      "Episode 1300, Total Reward: 273.86\n",
      "Episode 1310, Total Reward: 312.74\n",
      "Episode 1320, Total Reward: 166.31\n",
      "Episode 1330, Total Reward: 146.66\n",
      "Episode 1340, Total Reward: 186.69\n",
      "Episode 1350, Total Reward: 284.8\n",
      "Episode 1360, Total Reward: 186.88\n",
      "Episode 1370, Total Reward: 226.28\n",
      "Episode 1380, Total Reward: 304.09999999999997\n",
      "Episode 1390, Total Reward: 226.04\n",
      "Episode 1400, Total Reward: 264.70000000000005\n",
      "Episode 1410, Total Reward: 274.90999999999997\n",
      "Episode 1420, Total Reward: 360.06\n",
      "Episode 1430, Total Reward: 254.83999999999997\n",
      "Episode 1440, Total Reward: 59.01\n",
      "Episode 1450, Total Reward: 362.13\n",
      "Episode 1460, Total Reward: 363.07\n",
      "Episode 1470, Total Reward: 166.96\n",
      "Episode 1480, Total Reward: 372.69\n",
      "Episode 1490, Total Reward: 216.25\n",
      "Episode 1500, Total Reward: 324.48\n",
      "Episode 1510, Total Reward: 352.68\n",
      "Episode 1520, Total Reward: 127.33000000000001\n",
      "Episode 1530, Total Reward: 107.93\n",
      "Episode 1540, Total Reward: 186.65\n",
      "Episode 1550, Total Reward: 274.23\n",
      "Episode 1560, Total Reward: 303.75\n",
      "Episode 1570, Total Reward: 460.21\n",
      "Episode 1580, Total Reward: 303.63\n",
      "Episode 1590, Total Reward: 78.44999999999999\n",
      "Episode 1600, Total Reward: 520.21\n",
      "Episode 1610, Total Reward: 363.92\n",
      "Episode 1620, Total Reward: 422.63\n",
      "Episode 1630, Total Reward: 323.22999999999996\n",
      "Episode 1640, Total Reward: 392.75\n",
      "Episode 1650, Total Reward: 401.84000000000003\n",
      "Episode 1660, Total Reward: 430.45\n",
      "Episode 1670, Total Reward: 421.09000000000003\n",
      "Episode 1680, Total Reward: 381.81\n",
      "Episode 1690, Total Reward: 186.74999999999997\n",
      "Episode 1700, Total Reward: 146.94000000000003\n",
      "Episode 1710, Total Reward: 556.72\n",
      "Episode 1720, Total Reward: 362.46999999999997\n",
      "Episode 1730, Total Reward: 302.76000000000005\n",
      "Episode 1740, Total Reward: 421.82\n",
      "Episode 1750, Total Reward: -0.3200000000000002\n",
      "Episode 1760, Total Reward: 303.76\n",
      "Episode 1770, Total Reward: 48.78\n",
      "Episode 1780, Total Reward: 313.03000000000003\n",
      "Episode 1790, Total Reward: 226.16\n",
      "Episode 1800, Total Reward: 195.63\n",
      "Episode 1810, Total Reward: 301.05\n",
      "Episode 1820, Total Reward: 253.82999999999998\n",
      "Episode 1830, Total Reward: 406.71999999999997\n",
      "Episode 1840, Total Reward: 400.54\n",
      "Episode 1850, Total Reward: 312.24\n",
      "Episode 1860, Total Reward: 117.5\n",
      "Episode 1870, Total Reward: 208.64000000000001\n",
      "Episode 1880, Total Reward: 282.6\n",
      "Episode 1890, Total Reward: 330.34\n",
      "Episode 1900, Total Reward: 132.89000000000001\n",
      "Episode 1910, Total Reward: 327.39\n",
      "Episode 1920, Total Reward: 300.7\n",
      "Episode 1930, Total Reward: 284.18\n",
      "Episode 1940, Total Reward: 232.71\n",
      "Episode 1950, Total Reward: 340.91\n",
      "Episode 1960, Total Reward: 403.46\n",
      "Episode 1970, Total Reward: 322.99\n",
      "Episode 1980, Total Reward: 371.22\n",
      "Episode 1990, Total Reward: 429.78\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = game.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Collect experience\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Convert state to tensor\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Get action probabilities\n",
    "        action_probs = policy_network(state_tensor)\n",
    "        \n",
    "        # Sample action from probability distribution\n",
    "        action_distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _ = game.step(action.item())\n",
    "        \n",
    "        # Store reward and log probability\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(action_distribution.log_prob(action))\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate returns (discounted rewards)\n",
    "    returns = []\n",
    "    R = 0\n",
    "\n",
    "    # Calculate the total return over all time steps. We do reversed because we value the most things closer to time t more (they get multiplied by gamma less)\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    \n",
    "    # Normalize returns (optional but helps with training stability)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "    \n",
    "    # Calculate loss\n",
    "    policy_loss = []\n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)  # Negative because we're doing gradient ascent\n",
    "    \n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    \n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print episode results\n",
    "    total_reward = sum(rewards)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to 'snake_policy_weights.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model weights\n",
    "torch.save(policy_network.state_dict(), 'snake_policy_weights.pth')\n",
    "print(\"Model weights saved to 'snake_policy_weights.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 16:22:09.599 python[96806:1655281] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-11 16:22:09.599 python[96806:1655281] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 0, Total Reward: 254.2800000000014\n"
     ]
    }
   ],
   "source": [
    "# Test the trained policy\n",
    "visual_game = SnakeGame(render=True)\n",
    "\n",
    "def test_policy(policy, visual_game, episodes=1):\n",
    "    for episode in range(episodes):\n",
    "        state = visual_game.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = torch.argmax(action_probs, dim=1).item()\n",
    "            \n",
    "            state, reward, done, _ = visual_game.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "        print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "test_policy(policy_network, visual_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
