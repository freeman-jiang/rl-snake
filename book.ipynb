{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network for policy\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99  # Discount factor\n",
    "episodes = 2_000\n",
    "\n",
    "# Snake game\n",
    "input_dim = 12\n",
    "output_dim = 4\n",
    "\n",
    "# initialize policy network\n",
    "policy_network = PolicyNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = [danger + dir_one_hot + food_dir]\n",
    "from game import SnakeGame\n",
    "\n",
    "game = SnakeGame(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -0.6700000000000007\n",
      "Episode 10, Total Reward: -11.52\n",
      "Episode 20, Total Reward: -10.6\n",
      "Episode 30, Total Reward: -10.67\n",
      "Episode 40, Total Reward: -10.88\n",
      "Episode 50, Total Reward: -10.38\n",
      "Episode 60, Total Reward: -10.38\n",
      "Episode 70, Total Reward: -11.61\n",
      "Episode 80, Total Reward: -0.4000000000000007\n",
      "Episode 90, Total Reward: -10.32\n",
      "Episode 100, Total Reward: -10.93\n",
      "Episode 110, Total Reward: 9.079999999999998\n",
      "Episode 120, Total Reward: -10.620000000000001\n",
      "Episode 130, Total Reward: -11.569999999999999\n",
      "Episode 140, Total Reward: -10.700000000000001\n",
      "Episode 150, Total Reward: -12.13\n",
      "Episode 160, Total Reward: -10.98\n",
      "Episode 170, Total Reward: -1.3099999999999994\n",
      "Episode 180, Total Reward: -10.38\n",
      "Episode 190, Total Reward: -1.6399999999999995\n",
      "Episode 200, Total Reward: -12.16\n",
      "Episode 210, Total Reward: -0.28999999999999926\n",
      "Episode 220, Total Reward: -1.1699999999999995\n",
      "Episode 230, Total Reward: -1.1299999999999994\n",
      "Episode 240, Total Reward: -11.08\n",
      "Episode 250, Total Reward: -0.17999999999999924\n",
      "Episode 260, Total Reward: -0.4599999999999995\n",
      "Episode 270, Total Reward: -10.27\n",
      "Episode 280, Total Reward: -0.7199999999999998\n",
      "Episode 290, Total Reward: -10.57\n",
      "Episode 300, Total Reward: -10.28\n",
      "Episode 310, Total Reward: -10.370000000000001\n",
      "Episode 320, Total Reward: 8.739999999999997\n",
      "Episode 330, Total Reward: -12.15\n",
      "Episode 340, Total Reward: -0.5899999999999995\n",
      "Episode 350, Total Reward: -10.34\n",
      "Episode 360, Total Reward: -10.57\n",
      "Episode 370, Total Reward: 8.41\n",
      "Episode 380, Total Reward: -10.68\n",
      "Episode 390, Total Reward: 9.319999999999999\n",
      "Episode 400, Total Reward: -0.9799999999999995\n",
      "Episode 410, Total Reward: 9.25\n",
      "Episode 420, Total Reward: -0.6999999999999996\n",
      "Episode 430, Total Reward: -10.77\n",
      "Episode 440, Total Reward: -0.5199999999999999\n",
      "Episode 450, Total Reward: -10.21\n",
      "Episode 460, Total Reward: 19.490000000000002\n",
      "Episode 470, Total Reward: -10.530000000000001\n",
      "Episode 480, Total Reward: -0.5500000000000002\n",
      "Episode 490, Total Reward: -10.3\n",
      "Episode 500, Total Reward: -0.9200000000000009\n",
      "Episode 510, Total Reward: -0.4200000000000008\n",
      "Episode 520, Total Reward: 28.55\n",
      "Episode 530, Total Reward: 8.59\n",
      "Episode 540, Total Reward: 8.559999999999999\n",
      "Episode 550, Total Reward: -1.0000000000000009\n",
      "Episode 560, Total Reward: -0.6100000000000009\n",
      "Episode 570, Total Reward: 9.54\n",
      "Episode 580, Total Reward: 19.279999999999998\n",
      "Episode 590, Total Reward: -0.22000000000000014\n",
      "Episode 600, Total Reward: 8.510000000000002\n",
      "Episode 610, Total Reward: -10.55\n",
      "Episode 620, Total Reward: -0.38000000000000017\n",
      "Episode 630, Total Reward: -10.36\n",
      "Episode 640, Total Reward: 18.13\n",
      "Episode 650, Total Reward: -10.2\n",
      "Episode 660, Total Reward: 9.540000000000001\n",
      "Episode 670, Total Reward: 9.530000000000001\n",
      "Episode 680, Total Reward: -0.39000000000000035\n",
      "Episode 690, Total Reward: -0.37000000000000016\n",
      "Episode 700, Total Reward: -0.3100000000000006\n",
      "Episode 710, Total Reward: 28.819999999999993\n",
      "Episode 720, Total Reward: 8.670000000000002\n",
      "Episode 730, Total Reward: -0.23999999999999944\n",
      "Episode 740, Total Reward: 27.999999999999996\n",
      "Episode 750, Total Reward: -0.6999999999999997\n",
      "Episode 760, Total Reward: 9.409999999999998\n",
      "Episode 770, Total Reward: 87.99\n",
      "Episode 780, Total Reward: 38.56999999999999\n",
      "Episode 790, Total Reward: 9.37\n",
      "Episode 800, Total Reward: 37.66\n",
      "Episode 810, Total Reward: 37.910000000000004\n",
      "Episode 820, Total Reward: 9.610000000000001\n",
      "Episode 830, Total Reward: 57.69\n",
      "Episode 840, Total Reward: -0.6400000000000003\n",
      "Episode 850, Total Reward: -0.23999999999999927\n",
      "Episode 860, Total Reward: 38.21000000000001\n",
      "Episode 870, Total Reward: 19.47\n",
      "Episode 880, Total Reward: -0.5499999999999995\n",
      "Episode 890, Total Reward: 29.28\n",
      "Episode 900, Total Reward: 9.28\n",
      "Episode 910, Total Reward: -0.2900000000000008\n",
      "Episode 920, Total Reward: 9.129999999999999\n",
      "Episode 930, Total Reward: 38.73\n",
      "Episode 940, Total Reward: 19.04\n",
      "Episode 950, Total Reward: 19.240000000000002\n",
      "Episode 960, Total Reward: 19.43\n",
      "Episode 970, Total Reward: 18.91\n",
      "Episode 980, Total Reward: 38.78\n",
      "Episode 990, Total Reward: 97.17\n",
      "Episode 1000, Total Reward: 19.61\n",
      "Episode 1010, Total Reward: 28.5\n",
      "Episode 1020, Total Reward: 48.67000000000001\n",
      "Episode 1030, Total Reward: 19.240000000000002\n",
      "Episode 1040, Total Reward: 57.68\n",
      "Episode 1050, Total Reward: 87.21999999999998\n",
      "Episode 1060, Total Reward: 38.77\n",
      "Episode 1070, Total Reward: 48.22\n",
      "Episode 1080, Total Reward: 9.13\n",
      "Episode 1090, Total Reward: 58.3\n",
      "Episode 1100, Total Reward: 29.47\n",
      "Episode 1110, Total Reward: 29.150000000000002\n",
      "Episode 1120, Total Reward: 97.59\n",
      "Episode 1130, Total Reward: 19.490000000000002\n",
      "Episode 1140, Total Reward: 67.86\n",
      "Episode 1150, Total Reward: 48.66\n",
      "Episode 1160, Total Reward: 87.73\n",
      "Episode 1170, Total Reward: 68.7\n",
      "Episode 1180, Total Reward: 77.97\n",
      "Episode 1190, Total Reward: 48.85\n",
      "Episode 1200, Total Reward: 58.76999999999999\n",
      "Episode 1210, Total Reward: 48.51\n",
      "Episode 1220, Total Reward: 87.56\n",
      "Episode 1230, Total Reward: 48.64\n",
      "Episode 1240, Total Reward: -0.16000000000000014\n",
      "Episode 1250, Total Reward: 19.21\n",
      "Episode 1260, Total Reward: 38.73\n",
      "Episode 1270, Total Reward: 68.23\n",
      "Episode 1280, Total Reward: 9.730000000000002\n",
      "Episode 1290, Total Reward: 108.22\n",
      "Episode 1300, Total Reward: 59.22\n",
      "Episode 1310, Total Reward: 68.19000000000001\n",
      "Episode 1320, Total Reward: 87.78\n",
      "Episode 1330, Total Reward: 29.53\n",
      "Episode 1340, Total Reward: 58.33\n",
      "Episode 1350, Total Reward: 97.64\n",
      "Episode 1360, Total Reward: 58.05\n",
      "Episode 1370, Total Reward: 19.200000000000003\n",
      "Episode 1380, Total Reward: -0.6399999999999999\n",
      "Episode 1390, Total Reward: 9.600000000000003\n",
      "Episode 1400, Total Reward: 156.46\n",
      "Episode 1410, Total Reward: 136.69000000000003\n",
      "Episode 1420, Total Reward: 166.38000000000002\n",
      "Episode 1430, Total Reward: 97.77000000000001\n",
      "Episode 1440, Total Reward: 146.78\n",
      "Episode 1450, Total Reward: 136.59\n",
      "Episode 1460, Total Reward: 77.36999999999999\n",
      "Episode 1470, Total Reward: 107.33\n",
      "Episode 1480, Total Reward: 29.200000000000003\n",
      "Episode 1490, Total Reward: 58.05\n",
      "Episode 1500, Total Reward: 166.54999999999998\n",
      "Episode 1510, Total Reward: 97.73\n",
      "Episode 1520, Total Reward: 68.65\n",
      "Episode 1530, Total Reward: 185.86\n",
      "Episode 1540, Total Reward: 48.779999999999994\n",
      "Episode 1550, Total Reward: 137.71\n",
      "Episode 1560, Total Reward: 175.75\n",
      "Episode 1570, Total Reward: 78.43\n",
      "Episode 1580, Total Reward: 175.63\n",
      "Episode 1590, Total Reward: 107.69\n",
      "Episode 1600, Total Reward: 19.45\n",
      "Episode 1610, Total Reward: 28.7\n",
      "Episode 1620, Total Reward: 244.59\n",
      "Episode 1630, Total Reward: 245.35\n",
      "Episode 1640, Total Reward: 88.07000000000001\n",
      "Episode 1650, Total Reward: 78.42999999999999\n",
      "Episode 1660, Total Reward: 77.88\n",
      "Episode 1670, Total Reward: 9.330000000000002\n",
      "Episode 1680, Total Reward: 126.44\n",
      "Episode 1690, Total Reward: 118.03000000000002\n",
      "Episode 1700, Total Reward: 235.47\n",
      "Episode 1710, Total Reward: 117.42999999999999\n",
      "Episode 1720, Total Reward: 184.85000000000002\n",
      "Episode 1730, Total Reward: 29.31\n",
      "Episode 1740, Total Reward: 195.41\n",
      "Episode 1750, Total Reward: 196.07\n",
      "Episode 1760, Total Reward: 39.42\n",
      "Episode 1770, Total Reward: 68.13\n",
      "Episode 1780, Total Reward: 274.37\n",
      "Episode 1790, Total Reward: 68.17999999999999\n",
      "Episode 1800, Total Reward: 78.21999999999998\n",
      "Episode 1810, Total Reward: 58.919999999999995\n",
      "Episode 1820, Total Reward: 39.18\n",
      "Episode 1830, Total Reward: 48.52\n",
      "Episode 1840, Total Reward: 205.11\n",
      "Episode 1850, Total Reward: 204.82000000000002\n",
      "Episode 1860, Total Reward: 156.42000000000002\n",
      "Episode 1870, Total Reward: 214.89\n",
      "Episode 1880, Total Reward: 146.96\n",
      "Episode 1890, Total Reward: 38.99999999999999\n",
      "Episode 1900, Total Reward: 186.12999999999997\n",
      "Episode 1910, Total Reward: 87.8\n",
      "Episode 1920, Total Reward: 48.589999999999996\n",
      "Episode 1930, Total Reward: 186.34\n",
      "Episode 1940, Total Reward: 106.84\n",
      "Episode 1950, Total Reward: 29.38\n",
      "Episode 1960, Total Reward: 58.62\n",
      "Episode 1970, Total Reward: 68.50000000000001\n",
      "Episode 1980, Total Reward: 118.19\n",
      "Episode 1990, Total Reward: 205.84\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = game.reset()\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Collect experience\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Convert state to tensor\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Get action probabilities\n",
    "        action_probs = policy_network(state_tensor)\n",
    "        \n",
    "        # Sample action from probability distribution\n",
    "        action_distribution = torch.distributions.Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _ = game.step(action.item())\n",
    "        \n",
    "        # Store reward and log probability\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(action_distribution.log_prob(action))\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Calculate returns (discounted rewards)\n",
    "    returns = []\n",
    "    R = 0\n",
    "\n",
    "    # Calculate the total return over all time steps. We do reversed because we value the most things closer to time t more (they get multiplied by gamma less)\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.FloatTensor(returns)\n",
    "    \n",
    "    # Normalize returns (optional but helps with training stability)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "    \n",
    "    # Calculate loss\n",
    "    policy_loss = []\n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)  # Negative because we're doing gradient ascent\n",
    "    \n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    \n",
    "    # Update policy\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print episode results\n",
    "    total_reward = sum(rewards)\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to 'snake_policy_weights.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model weights\n",
    "torch.save(policy_network.state_dict(), 'snake_policy_weights.pth')\n",
    "print(\"Model weights saved to 'snake_policy_weights.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 15:13:40.847 python[78884:1391594] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-11 15:13:40.847 python[78884:1391594] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 0, Total Reward: 177.22000000000008\n",
      "Test Episode 1, Total Reward: 304.830000000002\n",
      "Test Episode 2, Total Reward: 186.4400000000008\n",
      "Test Episode 3, Total Reward: 323.88000000000204\n",
      "Test Episode 4, Total Reward: 244.21000000000168\n"
     ]
    }
   ],
   "source": [
    "# Test the trained policy\n",
    "visual_game = SnakeGame(render=True)\n",
    "\n",
    "def test_policy(policy, visual_game, episodes=1):\n",
    "    for episode in range(episodes):\n",
    "        state = visual_game.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = torch.argmax(action_probs, dim=1).item()\n",
    "            \n",
    "            state, reward, done, _ = visual_game.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "        print(f\"Test Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "test_policy(policy_network, visual_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
